# tasks file for ansible-role-apache-spark
- name: Wait for nodes to become reachable
  wait_for_connection:
    timeout: 120

- name: update packages
  become: true
  shell: "sudo yum update -y"

- name: Add spark user to wheel
  user:
    name: "{{ spark_user }}"
    state: present
    groups: "wheel"

- name: sudo without password for sudoers group
  copy:
    content: "%wheel ALL=(ALL:ALL) NOPASSWD:ALL"
    dest: /etc/sudoers.d/wheel_nopasswd
    mode: 0440

- name: Make sure ssh directory exists
  file:
    path: "/home/{{ spark_user }}/.ssh"
    state: directory

- name: Copy ssh public key
  copy:
    src: "{{ ssh_public_key_path }}"
    dest: "/home/{{ spark_user }}/.ssh/id_rsa.pub"
    owner: "{{ spark_user }}"
    mode: 0600

- name: add ssh public key to authorized users
  shell: "cat /home/{{ spark_user }}/.ssh/id_rsa.pub >> /home/{{ spark_user }}/.ssh/authorized_keys"

- name: install packages
  become: true
  yum:
    name: "{{ item }}"
    state: present
  with_items:
    - java-1.8.0-openjdk
    - unzip
    - tar

- name: Download spark using get_url
  become: true
  get_url:
    url: https://dlcdn.apache.org/spark/spark-{{ spark_version }}/{{ spark_artifact }}
    dest: /opt
    checksum: "{{ spark_version_checksum }}"

- name: Extract Spark archive
  become: true
  unarchive:
    src: "/opt/{{ spark_artifact }}"
    dest: /opt
    remote_src: yes

- name: stop spark worker
  become: true
  shell: "sbin/stop-worker.sh"
  args:
    chdir: "{{ spark_install_dir }}"
  ignore_errors: true
  when: inventory_hostname in groups['workers']

- name: stop spark master
  become: true
  shell: "sbin/stop-master.sh"
  args:
    chdir: "{{ spark_install_dir }}"
  ignore_errors: true
  when: inventory_hostname in groups['masters']

- name: Make sure installation directory is empty
  become: true
  file:
    path: "{{ spark_install_dir }}"
    state: absent

- name: Rename Spark archive
  become: true
  command: "mv -f /opt/{{ spark_artifact_extracted }} {{ spark_install_dir }}"

- name: Make sure log directory is present
  become: true
  file:
    path: "{{ spark_log_dir }}"
    state: directory
    owner: "{{ spark_user }}"

- name: start firewall
  become: true
  yum:
    name: firewalld
    state: present

- name: Open Spark ports
  become: yes
  firewalld:
    port: "{{ item }}"
    permanent: true
    immediate: true
    state: enabled
  with_items:
    - 7077/tcp
    - 4040/tcp
    - 8080/tcp
    - 8081/tcp

- name: set spark env
  shell: "{{ item }}"
  with_items:
    - "cp {{ spark_install_dir }}/conf/spark-env.sh.template {{ spark_install_dir }}/conf/spark-env.sh"
    - "echo SPARK_MASTER_HOST={{ master_ip_address }} >> {{ spark_install_dir }}/conf/spark-env.sh"

- name: start spark master
  become: true
  shell: "sbin/start-master.sh"
  args:
    chdir: "{{ spark_install_dir }}"
  when: inventory_hostname in groups['masters']

- name: start spark worker
  become: true
  shell: "sbin/start-worker.sh spark://{{ master_ip_address }}:7077"
  args:
    chdir: "{{ spark_install_dir }}"
  when: inventory_hostname in groups['workers']

- name: Set SPARK_HOME
  become: true
  template:
    src: spark_home.sh.j2
    dest: /etc/profile.d/spark_home.sh
